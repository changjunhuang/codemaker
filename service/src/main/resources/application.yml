#Tomcat
server:
  port: 8080

# Spring
spring:
  application:
    name: codemaker-service

  datasource:
#    type: com.alibaba.druid.pool.DruidDataSource
#    druid: ## druid数据库连接池的基本初始化属性
#      initial-size: 5 # 连接池初始化的大小
#      min-idle: 1 # 最小空闲的线程数
#      max-active: 20 # 最大活动的线程数

    # hikari连接池
    type: com.zaxxer.hikari.HikariDataSource
    hikari:
      minimum-idle: 0
      maximum-pool-size: 20
      connection-timeout: 30000
      idle-timeout: 10000
      auto-commit: true
      pool-name: HikariCP
      connection-test-query: SELECT 1

    mysql-master: ## 主库写
      driver-class-name: com.mysql.cj.jdbc.Driver
      jdbc-url: jdbc:mysql://localhost:3306/self_test?useUnicode=true&characterEncoding=UTF-8&serverTimezone=UTC
      username: root
      password: 10010hcj

    mysql-slave:  ## 从库读
      driver-class-name: com.mysql.cj.jdbc.Driver
      jdbc-url: jdbc:mysql://localhost:3306/duplicate_db?useUnicode=true&characterEncoding=UTF-8&serverTimezone=UTC
      username: root
      password: 10010hcj

# Redis
  redis:
    host: 127.0.0.1 #单机配置
    post: 6379 #单机配置
    password:
    #    cluster: #集群配置
    #      host:
    timeout: 1000
    database: 0
    lettuce:
      pool:
        max-active: 8  # 连接池最大连接数 默认8 ，负数表示没有限制
        max-idle: 8  # 连接池中的最大空闲连接 默认8
        max-wait: -1 # 连接池最大阻塞等待时间（使用负值表示没有限制） 默认-1
        min-idle: 0  # 连接池中的最小空闲连接 默认0

# Kafka
  kafka:
    bootstrap-servers: 127.0.0.1:9092  # Kafka的服务器地址
    listener:
      missing-topics-fatal: false

    producer: # producer 生产者
      retries: 0 # 重试次数
      acks: 1 # 应答级别:多少个分区副本备份完成时向生产者发送ack确认(可选0、1、all/-1)
      batch-size: 16384 # 批量大小
      buffer-memory: 33554432 # 生产端缓冲区大小
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      #      value-serializer: com.itheima.demo.config.MySerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer

    consumer: # consumer消费者
      group-id: kafka-group # 默认的消费组ID
      enable-auto-commit: true # 是否自动提交offset
      auto-commit-interval: 100  # 提交offset延时(接收到消息后多久提交offset)

      # earliest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
      # latest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
      # none:topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
      auto-offset-reset: latest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      #      value-deserializer: com.itheima.demo.config.MyDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer

# Activity
  activiti:
#    false:默认，数据库表不变，但是如果版本不对或者缺失表会抛出异常（生产使用）
#    true:表不存在，自动创建（开发使用）
#    create_drop: 启动时创建，关闭时删除表（测试使用）
#    drop_create: 启动时删除表,在创建表 （不需要手动关闭引擎）
    database-schema-update: true
    database-schema: activiti_test
#    none：不保存任何历史数据，流程中这是最高效的
#    activity：只保存流程实例和流程行为
#    audit：除了activity，还保存全部的流程任务以及其属性，audit为history默认值
#    full：除了audit、还保存其他全部流程相关的细节数据，包括一些流程参数
    history-level: full
    db-history-used: true  # 检查历史表是否存在，activities7 默认不开启历史表
    db-identity-used: false # 检查身份信息表是否存在
    check-process-definitions: false # 自动部署验证设置:true-开启（默认）、false-关闭
    deployment-mode: never-fail # 关闭自动部署
    async-executor-activate: false # 解决频繁查询SQL问题
    job-executor-activate: false #启用异步执行器

# ElasticSearch
  elasticsearch:
    rest:
      uris: http://localhost:9200 # 设置Elastic Search的连接地址
    username: elastic # 设置Elastic Search的用户名
    password: 10010hcj # 设置Elastic Search的密码
    connection-request-timeout: 5000 # 设置连接请求超时时间
    socket-timeout: 5000 # 设置Socket超时时间
    max-connections: 100 # 设置最大连接数
    max-connections-per-route: 10 # 设置每个路由的最大连接数
    hostname: 127.0.0.1
    port: 9200

# Drools
    drools:
      mode: stream  # 也可以指定全局的mode，选择stream或cloud（默认stream模式）
      auto-update: on # 自动更新，on 或 off（默认开启）
      update: 10  # 指定规则文件自动更新的周期，单位秒（默认30秒扫描偶一次）
      listener: on  # 规则监听日志，on 或 off（默认开启）
      verify: on # 开启 drl 语法检查，on 或 off（默认关闭）
      path: classpath:rules/*.drl # 配置drools规则文件路径，路径错误的话，启动会报错


# Mybatis
mybatis-plus:
  mapper-locations: classpath*:/mybatis/mapper/*.xml,/mybatis/*.xml  # 配置MyBatis-Plus扫描Mapper文件的位置
  type-aliases-package: com.self.codemaker.model  # 创建别名的类所在的包
  configuration:
    log-impl: org.apache.ibatis.logging.stdout.StdOutImpl #开启sql日志
    map-underscore-to-camel-case: true  # 该配置就是将带有下划线的表字段映射为驼峰格式的实体类属性

# Eureka
eureka:
  client:
    service-url:
      defaultZone: http://localhost:8088/eureka

# log
logging:
  pattern:
    console: '%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread]%-5level-%highlight(%C-%M:%L)-%msg%n'
  level:
    com.self.codemaker.dao: DEBUG # 包路径为mapper文件包路径